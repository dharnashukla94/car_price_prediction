# -*- coding: utf-8 -*-
"""Car_price.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/dharnashukla94/Used_Car_Price_Prediction/blob/master/Car_price.ipynb
"""

# import numpy as np 
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

# preperation
from sklearn.model_selection import train_test_split,KFold
from sklearn import preprocessing
from sklearn import model_selection
from sklearn.preprocessing import binarize, LabelEncoder, MinMaxScaler
from sklearn.feature_selection import SelectKBest, f_regression,SelectFromModel
from sklearn.linear_model import Ridge


# models
import random
from sklearn import metrics
from sklearn.ensemble import RandomForestRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC 
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor, AdaBoostClassifier
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
# Evaluation 
import math
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.metrics import mean_squared_error

random.seed(8)

# Loading the dataset into Pandas
    

path = '/content/drive/MyDrive/autos.csv'
data_df = pd.read_csv(path , encoding="ISO-8859-1")


#Pandas: whats the data row count?
data_df.shape
    
#Pandas: whats the distribution of the data?
data_df.describe()
    
#Pandas: What types of data do i have?
data_df.info()

data_df.head(20)

#data_df['offerType'].unique()
#data_df['seller'].unique()
#data_df['gearbox'].unique()
#data_df['notRepairedDamage'].unique()
#data_df['abtest'].unique()
#data_df['vehicleType'].unique()
#data_df['model'].unique()

# Replacing German values to english

data_df['offerType'] = data_df['offerType'].replace(['Angebot', 'Gesuch'], ['offer','Application'])
data_df['seller'] = data_df['seller'].replace(['privat','gewerblich'],['private' ,'commercial'])
data_df['gearbox'] = data_df['gearbox'].replace(['manuell' ,'automatik'] , ['manual' ,'autombile'])
data_df['notRepairedDamage']  = data_df['notRepairedDamage'].replace(['ja' ,'nein'] , ['yes' ,'no'])
data_df['vehicleType'] = data_df['vehicleType'].replace(['kleinwagen' , 'kombi' ,'andere'] , 
                                                        ['small car' , 'combi', 'others'])

data_df['model'] = (data_df['model'].replace({'reihe':'range' , 'klasse':'class'}
                                             , regex=True))

# # Dropping unnessary columns

data_df = data_df.drop(['lastSeen'], axis= 1)
data_df = data_df.drop(['dateCrawled'], axis= 1)
data_df = data_df.drop(['dateCreated'], axis= 1)

# checking if there exists any NULL values in the train set

total = data_df.isnull().sum().sort_values(ascending=False)
percent = (data_df.isnull().sum()/data_df.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)
print(missing_data)

data_df["gearbox"].value_counts()

data_df["brand"].isnull().sum()

data_df.groupby("brand")["gearbox"].value_counts()

gearbox = data_df["gearbox"].unique()
brand = data_df["brand"].unique()
d = {}

for i in brand :
    m = 0
    for j in gearbox :
        if data_df[(data_df.gearbox == j) & (data_df.brand == i)].shape[0] > m :
            m = data_df[(data_df.gearbox == j) & (data_df.brand == i)].shape[0]
            d[i] = j

for i in brand :
    data_df.loc[(data_df.brand == i) & (data_df.gearbox.isnull()) ,"gearbox" ] = d[i]

# no nans in gearbox
data_df["gearbox"].isnull().sum()

data_df["notRepairedDamage"].value_counts()

data_df["notRepairedDamage"].isnull().sum()

data_df["notRepairedDamage"].fillna("no",inplace = True)

data_df["notRepairedDamage"].isnull().sum()

data_df["fuelType"].value_counts()

data_df["fuelType"].fillna("benzin",inplace = True)

data_df.isnull().sum()

data_df["vehicleType"].value_counts()

data_df.groupby("fuelType")["vehicleType"].value_counts()

vehicleType = data_df["vehicleType"].unique()
fuelType = data_df["fuelType"].unique()
print(fuelType)
print(vehicleType)
#remove nan 
vehicleType = np.delete(vehicleType,0)

d = {}
for i in fuelType :
    m = 0
    for j in vehicleType :
        if data_df[(data_df.vehicleType == j) & (data_df.fuelType == i)].shape[0] > m :
            m = data_df[(data_df.vehicleType == j) & (data_df.fuelType == i)].shape[0]
            d[i] = j

for i in fuelType :
    data_df.loc[(data_df.fuelType == i) & (data_df.vehicleType.isnull()) ,"vehicleType" ] = d[i]

data_df["vehicleType"].isnull().sum()

len(data_df["model"].unique())

data_df["model"].unique()[0]

data_df["model"].fillna("golf",inplace =True)

data_df.isnull().sum()

data_df.head()

from sklearn.preprocessing import LabelEncoder

data = data_df.copy()

data["vehicleType"] =LabelEncoder().fit_transform(data["vehicleType"])
data["fuelType"] =LabelEncoder().fit_transform(data["fuelType"])
data["gearbox"] =LabelEncoder().fit_transform(data["gearbox"])
data["notRepairedDamage"] =LabelEncoder().fit_transform(data["notRepairedDamage"])
data["brand"] =LabelEncoder().fit_transform(data["brand"])
data["model"] =LabelEncoder().fit_transform(data["model"])
data["abtest"] =LabelEncoder().fit_transform(data["abtest"])
data["vehicleType"] =LabelEncoder().fit_transform(data["vehicleType"])
data["fuelType"] =LabelEncoder().fit_transform(data["fuelType"])
data["gearbox"] =LabelEncoder().fit_transform(data["gearbox"])
data["notRepairedDamage"] =LabelEncoder().fit_transform(data["notRepairedDamage"])
data["brand"] =LabelEncoder().fit_transform(data["brand"])
data["model"] =LabelEncoder().fit_transform(data["model"])
data["abtest"] =LabelEncoder().fit_transform(data["abtest"])
data['postalCode']=LabelEncoder().fit_transform(data['postalCode'])
data['seller']=LabelEncoder().fit_transform(data['seller'])
data['nrOfPictures']=LabelEncoder().fit_transform(data['nrOfPictures'])
data['monthOfRegistration']=LabelEncoder().fit_transform(data['monthOfRegistration'])
data['offerType']=LabelEncoder().fit_transform(data['offerType'])
data['name']=LabelEncoder().fit_transform(data['name'])
data["price"] = LabelEncoder().fit_transform(data['price'])

# analysis year o registration
data["yearOfRegistration"].describe()

data[data.yearOfRegistration > 2017].shape

data[data.yearOfRegistration < 1950].shape

data = data[(data.yearOfRegistration < 2017)  & (data.yearOfRegistration > 1950)]

data["price"].describe()

data[data.price < 100].shape

data[data.price > 200000].shape

data = data[(data.price > 100) & (data.price < 200000) ]

corr_matrix = data.corr(method="pearson")
plt.figure(figsize=(15, 15))
sns.heatmap(corr_matrix, vmin=-1., vmax=1., annot=True, fmt='.2f', cmap="YlGnBu", cbar=True, linewidths=0.5)
plt.title("pearson correlation")

X = data.drop("price", axis=1).values
y = data["price"].values
feature_names = data.drop("price", axis=1).columns
## p-value
selector = SelectKBest(f_regression, k=10).fit(X,y)
pvalue_selected_features = feature_names[selector.get_support()]

## regularization
selector = SelectFromModel(estimator = Ridge(alpha=1.0, fit_intercept=True), 
                                 max_features=10).fit(X,y)
regularization_selected_features = feature_names[selector.get_support()]
 
## plot
dtf_features = pd.DataFrame({"features":feature_names})
dtf_features["p_value"] = dtf_features["features"].apply(lambda x: "p_value" if x in pvalue_selected_features else "")
dtf_features["num1"] = dtf_features["features"].apply(lambda x: 1 if x in pvalue_selected_features else 0)
dtf_features["regularization"] = dtf_features["features"].apply(lambda x: "regularization" if x in regularization_selected_features else "")
dtf_features["num2"] = dtf_features["features"].apply(lambda x: 1 if x in regularization_selected_features else 0)
dtf_features["method"] = dtf_features[["p_value","regularization"]].apply(lambda x: (x[0]+" "+x[1]).strip(), axis=1)
dtf_features["selection"] = dtf_features["num1"] + dtf_features["num2"]
dtf_features["method"] = dtf_features["method"].apply(lambda x: "both" if len(x.split()) == 2 else x)
sns.barplot(y="features", x="selection", hue="method", data=dtf_features.sort_values("selection", ascending=False), dodge=False)

x_ = "price"
fig, ax = plt.subplots(nrows=1, ncols=2,  sharex=False, sharey=False)
fig.suptitle(x_, fontsize=20)
### distribution
ax[0].title.set_text('distribution')
variable = data[x_].fillna(data[x_].mean())
breaks = np.quantile(variable, q=np.linspace(0, 1, 11))
variable = variable[ (variable > breaks[0]) & (variable < 
                    breaks[10]) ]
sns.distplot(variable, hist=True, kde=True, kde_kws={"shade": True}, ax=ax[0])
des = data[x_].describe()
ax[0].axvline(des["25%"], ls='--')
ax[0].axvline(des["mean"], ls='--')
ax[0].axvline(des["75%"], ls='--')
ax[0].grid(True)
des = round(des, 2).apply(lambda x: str(x))
box = '\n'.join(("min: "+des["min"], "25%: "+des["25%"], "mean: "+des["mean"], "75%: "+des["75%"], "max: "+des["max"]))
ax[0].text(0.95, 0.95, box, transform=ax[0].transAxes, fontsize=10, va='top', ha="right", bbox=dict(boxstyle='round', facecolor='white', alpha=1))
### boxplot 
ax[1].title.set_text('outliers (log scale)')
tmp_dtf = pd.DataFrame(data[x_])
tmp_dtf[x_] = np.log(tmp_dtf[x_])
tmp_dtf.boxplot(column=x_, ax=ax[1])
plt.show()

x_cols = ['gearbox', 'fuelType', 'notRepairedDamage']

y  = data["price"]
x =  data[x_cols]

# scaling the data
from sklearn.preprocessing import StandardScaler

data_scale = x
data_scale['price'] = y
scaler = StandardScaler()
data_scale = scaler.fit_transform(data_scale)

data_scale = pd.DataFrame(data_scale, columns = x.columns)

data_scale

# Checking of outliers in FuelType 
cat, num = "fuelType", "price"
ax[0].title.set_text('outliers')
sns.catplot(x=cat, y=num, data=data, kind="box", ax=ax[0])
ax[0].grid(True)
plt.show()

# Checking of outliers in notRepairedDamage 
cat, num = "notRepairedDamage", "price"
ax[0].title.set_text('outliers')
sns.catplot(x=cat, y=num, data=data, kind="box", ax=ax[0])
ax[0].grid(True)
plt.show()

# Checking of outliers in gearbox 
cat, num = "gearbox", "price"
ax[0].title.set_text('outliers')
sns.catplot(x=cat, y=num, data=data, kind="box", ax=ax[0])
ax[0].grid(True)
plt.show()

y_new  = data_scale["price"]
x_new =  data_scale[x_cols]

# checking skew of the function

print(x_new['notRepairedDamage'].skew())
x_new['notRepairedDamage'].describe()

print(x_new['gearbox'].skew())
x_new['gearbox'].describe()

print(x_new['fuelType'].skew())
x_new['fuelType'].describe()

print(y_new.skew())
y_new.describe()

x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, random_state = 0)

import sklearn.metrics as metrics
def regression_results(y_test, y_pred):

    # Regression metrics

    mean_absolute_error=metrics.mean_absolute_error(y_test, y_pred) 
    mse=metrics.mean_squared_error(y_test, y_pred) 
    median_absolute_error=metrics.median_absolute_error(y_test, y_pred)
    
    print('MAE: ', round(mean_absolute_error,4))
    print('MSE: ', round(mse,4))
    print('RMSE: ', round(np.sqrt(mse),4))

# LinearRegressor
lin_model = LinearRegression()
lin_model.fit(x_train, y_train)
y_pred = lin_model.predict(x_test)

regression_results(y_test,y_pred)
## residuals
residuals = y_test - y_pred
fig, ax = plt.subplots()
sns.distplot(residuals, color="red", hist=True, kde=True, kde_kws={"shade":True}, ax=ax)
ax.grid(True)
ax.set(yticks=[], yticklabels=[], title="Residuals distribution")
plt.show()

# KnnRegressor
knn = KNeighborsRegressor()
knn.fit(x_train, y_train)
y_pred = knn.predict(x_test)
regression_results(y_test,y_pred)

## residuals
residuals = y_test - y_pred
fig, ax = plt.subplots()
sns.distplot(residuals, color="red", hist=True, kde=True, kde_kws={"shade":True}, ax=ax)
ax.grid(True)
ax.set(yticks=[], yticklabels=[], title="Residuals distribution")
plt.show()

# Random forest Regressor
random = RandomForestRegressor()
random.fit(x_train, y_train)
y_pred = random.predict(x_test)
regression_results(y_test,y_pred)

## residuals
residuals = y_test - y_pred
fig, ax = plt.subplots()
sns.distplot(residuals, color="red", hist=True, kde=True, kde_kws={"shade":True}, ax=ax)
ax.grid(True)
ax.set(yticks=[], yticklabels=[], title="Residuals distribution")
plt.show()

# Gradient Boosting Regressor
reg = GradientBoostingRegressor(random_state=0)
reg.fit(x_train, y_train)
y_pred = reg.predict(x_test)
regression_results(y_test,y_pred)

## residuals
residuals = y_test - y_pred
fig, ax = plt.subplots()
sns.distplot(residuals, color="red", hist=True, kde=True, kde_kws={"shade":True}, ax=ax)
ax.grid(True)
ax.set(yticks=[], yticklabels=[], title="Residuals distribution")
plt.show()

# saving the model

import pickle

# save the model to disk
filename = "final_model.pkl"  

with open(filename, 'wb') as file:  
    pickle.dump(reg, file)
 
 
# load the model from disk
#with open(filename, 'rb') as file:  
#    model = pickle.load(file)

# XGboost Regressor
xgb = XGBRegressor(objective='reg:squarederror')
xgb.fit(x_train, y_train)
y_pred = xgb.predict(x_test)
regression_results(y_test,y_pred)


## residuals
residuals = y_test - y_pred
fig, ax = plt.subplots()
sns.distplot(residuals, color="red", hist=True, kde=True, kde_kws={"shade":True}, ax=ax)
ax.grid(True)
ax.set(yticks=[], yticklabels=[], title="Residuals distribution")
plt.show()

# Multilayer perceptron 
from sklearn.neural_network import MLPRegressor

regr = MLPRegressor(random_state=1, max_iter=100).fit(x_train, y_train)
y_pred = regr.predict(x_test)
regression_results(y_test,y_pred)

## residuals
residuals = y_test - y_pred
fig, ax = plt.subplots()
sns.distplot(residuals, color="red", hist=True, kde=True, kde_kws={"shade":True}, ax=ax)
ax.grid(True)
ax.set(yticks=[], yticklabels=[], title="Residuals distribution")
plt.show()